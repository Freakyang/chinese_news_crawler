from flask import Flask, jsonify, request
from flask_sqlalchemy import SQLAlchemy
from datetime import datetime, timedelta
import os
import json
import threading
import time
import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse
import random

app = Flask(__name__)
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///news.db'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

db = SQLAlchemy(app)

# è³‡æ–™åº«æ¨¡å‹
class NewsArticle(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    title = db.Column(db.String(500), nullable=False)
    content = db.Column(db.Text)
    source = db.Column(db.String(100), nullable=False)
    url = db.Column(db.String(500), nullable=False)
    publish_date = db.Column(db.DateTime, nullable=False)
    topic = db.Column(db.String(100))
    keywords = db.Column(db.Text)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)

class Topic(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(200), nullable=False)
    keyword = db.Column(db.String(100), nullable=False)
    priority = db.Column(db.Integer, default=1)
    is_active = db.Column(db.Boolean, default=True)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)

# çœŸå¯¦æ–°èçˆ¬èŸ²é¡
class RealNewsCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def crawl_news(self, keyword, max_articles=20, start_date=None, end_date=None):
        """çˆ¬å–çœŸå¯¦æ–°è - ç´”å‹•æ…‹æœå°‹"""
        articles = []
        
        print(f"ğŸš€ é–‹å§‹å‹•æ…‹æœå°‹é—œéµè©: {keyword}")
        if start_date and end_date:
            print(f"ğŸ“… æœå°‹æ—¥æœŸç¯„åœ: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
        
        # é¦–å…ˆå˜—è©¦Googleæœå°‹
        try:
            print(f"ğŸ” æ­£åœ¨Googleæœå°‹é—œæ–¼ '{keyword}' çš„çœŸå¯¦æ–°è...")
            google_articles = self._crawl_google_news(keyword, max_articles, start_date, end_date)
            articles.extend(google_articles)
            print(f"âœ… å¾Googleæœå°‹ç²å¾— {len(google_articles)} ç¯‡çœŸå¯¦æ–°è")
        except Exception as e:
            print(f"âŒ Googleæœå°‹å¤±æ•—: {e}")
        
        # å¦‚æœGoogleæœå°‹çµæœä¸è¶³ï¼Œå˜—è©¦ä¸å¸¶æ—¥æœŸé™åˆ¶çš„æœå°‹
        if len(articles) < 5:
            print(f"ğŸ”„ Googleæœå°‹çµæœä¸è¶³ï¼Œå˜—è©¦ä¸å¸¶æ—¥æœŸé™åˆ¶çš„æœå°‹...")
            try:
                google_articles_no_date = self._crawl_google_news(keyword, max_articles - len(articles))
                articles.extend(google_articles_no_date)
                print(f"âœ… å¾ç„¡æ—¥æœŸé™åˆ¶æœå°‹ç²å¾— {len(google_articles_no_date)} ç¯‡çœŸå¯¦æ–°è")
            except Exception as e:
                print(f"âŒ ç„¡æ—¥æœŸé™åˆ¶æœå°‹å¤±æ•—: {e}")
        
        # å¦‚æœGoogleæœå°‹çµæœä¸è¶³ï¼Œå˜—è©¦å…¶ä»–æ–°èä¾†æº
        if len(articles) < max_articles:
            remaining = max_articles - len(articles)
            print(f"ğŸ”„ è£œå……å…¶ä»–æ–°èä¾†æºï¼Œé‚„éœ€è¦ {remaining} ç¯‡...")
            
            # æ–°èä¾†æºåˆ—è¡¨
            news_sources = [
                {
                    'name': 'Yahooæ–°è',
                    'search_url': f'https://tw.news.yahoo.com/search?p={keyword}',
                    'base_url': 'https://tw.news.yahoo.com'
                },
                {
                    'name': 'ETtodayæ–°èé›²',
                    'search_url': f'https://www.ettoday.net/news_search/doSearch.php?keywords={keyword}',
                    'base_url': 'https://www.ettoday.net'
                }
            ]
            
            for source in news_sources:
                if len(articles) >= max_articles:
                    break
                    
                try:
                    print(f"ğŸ” æ­£åœ¨çˆ¬å– {source['name']} é—œæ–¼ '{keyword}' çš„æ–°è...")
                    
                    # ç™¼é€è«‹æ±‚
                    response = self.session.get(source['search_url'], timeout=10)
                    response.encoding = 'utf-8'
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # æ ¹æ“šä¸åŒç¶²ç«™è§£ææ–°è
                        if 'yahoo' in source['name'].lower():
                            source_articles = self._parse_yahoo_news(soup, source, keyword)
                        elif 'ettoday' in source['name'].lower():
                            source_articles = self._parse_ettoday_news(soup, source, keyword)
                        else:
                            continue
                        
                        # é¿å…é‡è¤‡æ–‡ç« 
                        for article in source_articles:
                            if not any(existing['title'] == article['title'] for existing in articles):
                                articles.append(article)
                                if len(articles) >= max_articles:
                                    break
                    
                except Exception as e:
                    print(f"âŒ çˆ¬å– {source['name']} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    continue
        
        # å¦‚æœä»ç„¶æ²’æœ‰è¶³å¤ çš„æ–°èï¼Œå˜—è©¦æ›´å¤šæœå°‹ç­–ç•¥
        if len(articles) < 3:
            print("ğŸ”„ å˜—è©¦æ›´å¤šæœå°‹ç­–ç•¥...")
            try:
                # å˜—è©¦ä¸åŒçš„æœå°‹è©çµ„åˆ
                search_variations = [
                    f"{keyword} æœ€æ–°",
                    f"{keyword} ä»Šæ—¥",
                    f"{keyword} å³æ™‚"
                ]
                
                for variation in search_variations:
                    if len(articles) >= max_articles:
                        break
                        
                    print(f"ğŸ” å˜—è©¦æœå°‹: {variation}")
                    variation_articles = self._crawl_google_news(variation, 5, start_date, end_date)
                    
                    for article in variation_articles:
                        if not any(existing['title'] == article['title'] for existing in articles):
                            articles.append(article)
                            if len(articles) >= max_articles:
                                break
                                
            except Exception as e:
                print(f"âŒ é¡å¤–æœå°‹ç­–ç•¥å¤±æ•—: {e}")
        
        print(f"ğŸ‰ å‹•æ…‹æœå°‹å®Œæˆï¼ç¸½å…±ç²å¾— {len(articles)} ç¯‡çœŸå¯¦æ–°è")
        return articles[:max_articles]
    
    def _extract_date_from_article(self, url, title, content):
        """å¾URLã€æ¨™é¡Œæˆ–å…§å®¹ä¸­æå–ç™¼å¸ƒæ—¥æœŸ"""
        import re
        from datetime import datetime, timedelta

        # æ–¹æ³•1: å¾URLä¸­æå–æ—¥æœŸ (ä¾‹å¦‚: /news/20250929/...)
        url_date_match = re.search(r'/(\d{8})/', url)
        if url_date_match:
            date_str = url_date_match.group(1)
            try:
                return datetime.strptime(date_str, '%Y%m%d').date()
            except:
                pass

        # æ–¹æ³•2: å¾URLä¸­æå–æ—¥æœŸ (ä¾‹å¦‚: /2025/09/29/...)
        url_date_match2 = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', url)
        if url_date_match2:
            year, month, day = url_date_match2.groups()
            try:
                return datetime.strptime(f"{year}{month}{day}", '%Y%m%d').date()
            except:
                pass

        # æ–¹æ³•3: å¾æ¨™é¡Œæˆ–å…§å®¹ä¸­æå–æ—¥æœŸ
        date_patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',  # 2025å¹´9æœˆ29æ—¥
            r'(\d{4})/(\d{1,2})/(\d{1,2})',      # 2025/9/29
            r'(\d{4})-(\d{1,2})-(\d{1,2})',      # 2025-9-29
        ]

        text = title + ' ' + content
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                year, month, day = match.groups()
                try:
                    return datetime.strptime(f"{year}{month.zfill(2)}{day.zfill(2)}", '%Y%m%d').date()
                except:
                    continue

        # æ–¹æ³•4: å¾"Xå¤©å‰"ã€"Xå°æ™‚å‰"ç­‰ç›¸å°æ™‚é–“æå–
        relative_patterns = [
            r'(\d+)\s*å¤©å‰',  # Xå¤©å‰
            r'(\d+)\s*å°æ™‚å‰',  # Xå°æ™‚å‰
            r'(\d+)\s*åˆ†é˜å‰',  # Xåˆ†é˜å‰
        ]
        
        for pattern in relative_patterns:
            match = re.search(pattern, text)
            if match:
                value = int(match.group(1))
                if 'å¤©å‰' in match.group(0):
                    return (datetime.now() - timedelta(days=value)).date()
                elif 'å°æ™‚å‰' in match.group(0):
                    return (datetime.now() - timedelta(hours=value)).date()
                elif 'åˆ†é˜å‰' in match.group(0):
                    return (datetime.now() - timedelta(minutes=value)).date()

        # å¦‚æœç„¡æ³•æå–æ—¥æœŸï¼Œè¿”å›3å¤©å‰çš„æ—¥æœŸï¼ˆæ›´åˆç†çš„é è¨­å€¼ï¼‰
        fallback_date = (datetime.now() - timedelta(days=3)).date()
        print(f"âš ï¸ ç„¡æ³•å¾URLæˆ–å…§å®¹ä¸­æå–æ—¥æœŸï¼Œä½¿ç”¨é è¨­æ—¥æœŸ: {fallback_date}")
        return fallback_date
    
    def _crawl_google_news(self, keyword, max_articles=20, start_date=None, end_date=None):
        """å¾Googleæœå°‹çµæœçˆ¬å–çœŸå¯¦æ–°è"""
        articles = []
        try:
            # ä½¿ç”¨å¤šç¨®æœå°‹ç­–ç•¥ä¾†ç²å–æ›´å¤šæ–°è
            search_queries = [
                f"{keyword} æ–°è",
                f"{keyword} æœ€æ–°æ¶ˆæ¯",
                f"{keyword} å ±å°"
            ]
            
            for query in search_queries:
                if len(articles) >= max_articles:
                    break
                    
                search_url = f"https://www.google.com/search?q={query}&tbm=nws&num=20"
                print(f"ğŸ” æœå°‹: {query}")
                
                # ç™¼é€è«‹æ±‚
                response = self.session.get(search_url, timeout=15)
                response.encoding = 'utf-8'
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # æ›´ç²¾ç¢ºçš„Googleæœå°‹çµæœè§£æ
                    news_results = []
                    
                    # æ–¹æ³•1: å°‹æ‰¾æ‰€æœ‰åŒ…å«æ¨™é¡Œå’Œé€£çµçš„çµæœå®¹å™¨
                    results = soup.find_all('div', class_='g')
                    
                    # æ–¹æ³•2: å¦‚æœæ²’æœ‰æ‰¾åˆ°ï¼Œå˜—è©¦å…¶ä»–å¯èƒ½çš„å®¹å™¨
                    if not results:
                        results = soup.find_all('div', {'data-ved': True})
                    
                    # æ–¹æ³•3: å°‹æ‰¾æ‰€æœ‰h3æ¨™é¡Œçš„çˆ¶å®¹å™¨
                    if not results:
                        h3_elements = soup.find_all('h3')
                        for h3 in h3_elements:
                            parent = h3.find_parent('div')
                            if parent and parent not in results:
                                results.append(parent)
                    
                    news_results = results
                    
                    print(f"ğŸ” æ‰¾åˆ° {len(news_results)} å€‹æœå°‹çµæœ")
                    
                    for i, result in enumerate(news_results):
                        if len(articles) >= max_articles:
                            break
                            
                        try:
                            # æå–æ¨™é¡Œ - ä½¿ç”¨å¤šç¨®æ–¹æ³•
                            title = None
                            title_elem = result.find('h3')
                            if title_elem:
                                title = title_elem.get_text(strip=True)
                            
                            # å¦‚æœæ²’æœ‰æ‰¾åˆ°h3ï¼Œå˜—è©¦å…¶ä»–æ¨™é¡Œå…ƒç´ 
                            if not title:
                                title_elem = result.find('a')
                                if title_elem:
                                    title = title_elem.get_text(strip=True)
                            
                            if not title or len(title) < 5:
                                continue
                            
                            # æå–é€£çµ - ä½¿ç”¨å¤šç¨®æ–¹æ³•
                            url = None
                            link_elem = result.find('a')
                            if link_elem and link_elem.get('href'):
                                url = link_elem.get('href')
                            
                            if not url:
                                continue
                            
                            # è™•ç†Googleé‡å®šå‘URL
                            if url.startswith('/url?q='):
                                url = url.split('/url?q=')[1].split('&')[0]
                                url = url.replace('%3A', ':').replace('%2F', '/')
                            elif url.startswith('/search?') or url.startswith('/'):
                                continue
                            
                            # é©—è­‰URLæ˜¯å¦æœ‰æ•ˆ
                            if not url or not url.startswith('http'):
                                continue
                            
                            # æå–ä¾†æº
                            source = "Googleæœå°‹"
                            source_elem = result.find('cite')
                            if source_elem:
                                source = source_elem.get_text(strip=True)
                            
                            # æå–æ‘˜è¦
                            snippet = f"é—œæ–¼ {keyword} çš„æœ€æ–°æ¶ˆæ¯"
                            snippet_elem = result.find('span', class_='VwiC3b')
                            if not snippet_elem:
                                snippet_elem = result.find('div', class_='VwiC3b')
                            if snippet_elem:
                                snippet = snippet_elem.get_text(strip=True)
                            
                            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ¨™é¡Œçš„æ–‡ç« 
                            if any(existing['title'] == title for existing in articles):
                                continue
                            
                            # å˜—è©¦å¾URLæˆ–å…§å®¹ä¸­æå–å¯¦éš›ç™¼å¸ƒæ—¥æœŸ
                            article_date = self._extract_date_from_article(url, title, snippet)
                            
                            # æª¢æŸ¥æ—¥æœŸç¯©é¸ï¼ˆæ”¾å¯¬æ¢ä»¶ï¼Œå…è¨±3å¤©å…§çš„èª¤å·®ï¼‰
                            if start_date and end_date:
                                # è¨ˆç®—æ—¥æœŸç¯„åœï¼Œå…è¨±å‰å¾Œ3å¤©çš„èª¤å·®
                                from datetime import timedelta
                                extended_start = start_date.date() - timedelta(days=3)
                                extended_end = end_date.date() + timedelta(days=3)
                                
                                if not (extended_start <= article_date <= extended_end):
                                    print(f"âŒ æ–°èæ—¥æœŸ {article_date} ä¸åœ¨æœå°‹ç¯„åœå…§ï¼ˆå…è¨±Â±3å¤©èª¤å·®ï¼‰ï¼Œè·³é")
                                    continue
                                else:
                                    print(f"âœ… æ–°èæ—¥æœŸ {article_date} ç¬¦åˆæœå°‹ç¯„åœï¼ˆå…è¨±Â±3å¤©èª¤å·®ï¼‰")
                            
                            # å‰µå»ºæ–‡ç« ç‰©ä»¶
                            article = {
                                'title': title,
                                'content': snippet,
                                'source': source,
                                'url': url,
                                'publish_date': article_date,
                                'topic': self._classify_topic(title + ' ' + snippet),
                                'keywords': keyword
                            }
                            
                            articles.append(article)
                            print(f"ğŸ“° æ‰¾åˆ°çœŸå¯¦æ–°è: {title[:50]}...")
                            print(f"ğŸ”— é€£çµ: {url}")
                            
                        except Exception as e:
                            print(f"âŒ è§£æç¬¬ {i+1} å€‹æœå°‹çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                            continue
                
                print(f"âœ… å¾ '{query}' æœå°‹ç²å¾— {len(articles)} ç¯‡æ–°è")
            
            print(f"ğŸ‰ ç¸½å…±ç²å¾— {len(articles)} ç¯‡çœŸå¯¦æ–°è")
            return articles
            
        except Exception as e:
            print(f"âŒ Googleæœå°‹çˆ¬å–å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _parse_yahoo_news(self, soup, source, keyword):
        """è§£æYahooæ–°è"""
        articles = []
        try:
            # å°‹æ‰¾æ–°èé€£çµ
            news_links = soup.find_all('a', href=True)
            for link in news_links[:10]:  # é™åˆ¶æ•¸é‡
                href = link.get('href')
                if href and ('news' in href or 'story' in href):
                    title = link.get_text(strip=True)
                    if title and len(title) > 10:
                        full_url = urljoin(source['base_url'], href)
                        articles.append({
                            'title': title,
                            'content': f"é—œæ–¼ {keyword} çš„æœ€æ–°æ¶ˆæ¯",
                            'source': source['name'],
                            'url': full_url,
                            'publish_date': datetime.now(),
                            'topic': self._classify_topic(keyword),
                            'keywords': keyword
                        })
        except Exception as e:
            print(f"è§£æYahooæ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return articles
    
    def _parse_ettoday_news(self, soup, source, keyword):
        """è§£æETtodayæ–°è"""
        articles = []
        try:
            # å°‹æ‰¾æ–°èæ¨™é¡Œå’Œé€£çµ
            news_items = soup.find_all(['h3', 'h2', 'h1'], class_=re.compile(r'title|headline'))
            for item in news_items[:10]:
                link = item.find('a')
                if link:
                    title = link.get_text(strip=True)
                    href = link.get('href')
                    if title and href and len(title) > 10:
                        full_url = urljoin(source['base_url'], href)
                        articles.append({
                            'title': title,
                            'content': f"é—œæ–¼ {keyword} çš„æœ€æ–°å ±å°",
                            'source': source['name'],
                            'url': full_url,
                            'publish_date': datetime.now(),
                            'topic': self._classify_topic(keyword),
                            'keywords': keyword
                        })
        except Exception as e:
            print(f"è§£æETtodayæ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return articles
    
    def _parse_chinatimes_news(self, soup, source, keyword):
        """è§£æä¸­æ™‚æ–°èç¶²"""
        articles = []
        try:
            # å°‹æ‰¾æ–°èæ¨™é¡Œ
            news_items = soup.find_all(['h3', 'h2'], class_=re.compile(r'title|headline'))
            for item in news_items[:10]:
                link = item.find('a')
                if link:
                    title = link.get_text(strip=True)
                    href = link.get('href')
                    if title and href and len(title) > 10:
                        full_url = urljoin(source['base_url'], href)
                        articles.append({
                            'title': title,
                            'content': f"é—œæ–¼ {keyword} çš„é‡è¦æ–°è",
                            'source': source['name'],
                            'url': full_url,
                            'publish_date': datetime.now(),
                            'topic': self._classify_topic(keyword),
                            'keywords': keyword
                        })
        except Exception as e:
            print(f"è§£æä¸­æ™‚æ–°èç¶²æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return articles
    
    def _classify_topic(self, keyword):
        """æ ¹æ“šé—œéµè©åˆ†é¡ä¸»é¡Œ"""
        topic_keywords = {
            'æ”¿æ²»': ['æ”¿æ²»', 'é¸èˆ‰', 'æ”¿åºœ', 'ç¸½çµ±', 'ç«‹å§”', 'æ”¿é»¨'],
            'ç¶“æ¿Ÿ': ['ç¶“æ¿Ÿ', 'è‚¡å¸‚', 'é‡‘è', 'æŠ•è³‡', 'GDP', 'é€šè†¨'],
            'ç§‘æŠ€': ['ç§‘æŠ€', 'AI', 'äººå·¥æ™ºæ…§', '5G', 'åŠå°é«”', 'æ™¶ç‰‡'],
            'åœ‹éš›': ['åœ‹éš›', 'ç¾åœ‹', 'ä¸­åœ‹', 'æ—¥æœ¬', 'éŸ“åœ‹', 'æ­æ´²'],
            'ç¤¾æœƒ': ['ç¤¾æœƒ', 'æ°‘ç”Ÿ', 'æ•™è‚²', 'é†«ç™‚', 'äº¤é€š', 'ç’°ä¿'],
            'é«”è‚²': ['é«”è‚²', 'é‹å‹•', 'å¥§é‹', 'è¶³çƒ', 'ç±ƒçƒ', 'æ£’çƒ'],
            'å¨›æ¨‚': ['å¨›æ¨‚', 'é›»å½±', 'éŸ³æ¨‚', 'æ˜æ˜Ÿ', 'è—äºº', 'ç¶œè—']
        }
        
        for topic, keywords in topic_keywords.items():
            if any(kw in keyword for kw in keywords):
                return topic
        return 'ç¶œåˆ'
    
    # ç§»é™¤æ‰€æœ‰å‚™ç”¨æ–°èè³‡æ–™å‡½æ•¸ï¼Œæ”¹ç‚ºç´”å‹•æ…‹æœå°‹

@app.route('/')
def index():
    return '''
    <!DOCTYPE html>
    <html lang="zh-TW">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ä¸­æ–‡æ–°èçˆ¬èŸ²èˆ‡ä¸»é¡Œåˆ†æç³»çµ±</title>
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
        <style>
            body { background-color: #f8f9fa; font-family: 'Microsoft JhengHei', sans-serif; }
            .navbar-brand { font-weight: bold; color: #2c3e50 !important; }
            .card { border: none; box-shadow: 0 0.125rem 0.25rem rgba(0, 0, 0, 0.075); border-radius: 0.5rem; }
            .stats-card { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; }
            .stats-number { font-size: 2rem; font-weight: bold; }
            .news-item { 
                border-left: 4px solid #007bff; 
                padding: 15px; 
                margin-bottom: 15px; 
                background: white;
                border-radius: 8px;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                transition: transform 0.2s ease;
            }
            .news-item:hover { 
                transform: translateY(-2px);
                box-shadow: 0 4px 8px rgba(0,0,0,0.15);
            }
            .news-title { 
                color: #2c3e50; 
                font-weight: bold; 
                text-decoration: none; 
                font-size: 1.1rem;
                line-height: 1.4;
                display: block;
                margin-bottom: 8px;
            }
            .news-title:hover { 
                color: #007bff; 
                text-decoration: underline; 
            }
            .news-meta { 
                color: #6c757d; 
                font-size: 0.9rem; 
                margin-bottom: 8px;
            }
            .news-content { 
                color: #495057; 
                font-size: 0.95rem; 
                line-height: 1.5;
                margin-top: 8px;
            }
            .btn-crawl { background: linear-gradient(45deg, #28a745, #20c997); border: none; }
            .btn-crawl:hover { background: linear-gradient(45deg, #218838, #1ea085); }
            .loading { display: none; }
            .topic-badge { margin: 2px; }
            .news-list-container { 
                max-height: 600px; 
                overflow-y: auto; 
                padding-right: 10px;
            }
            .search-keyword { 
                background: linear-gradient(45deg, #ff6b6b, #ee5a24); 
                color: white; 
                padding: 4px 8px; 
                border-radius: 4px; 
                font-size: 0.8rem;
                margin-right: 8px;
            }
        </style>
    </head>
    <body>
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="#">ğŸŒ ä¸­æ–‡æ–°èçˆ¬èŸ²èˆ‡ä¸»é¡Œåˆ†æç³»çµ±</a>
            </div>
        </nav>

        <div class="container mt-4">
            <!-- çµ±è¨ˆå¡ç‰‡ -->
            <div class="row mb-4">
                <div class="col-md-3">
                    <div class="card stats-card">
                        <div class="card-body text-center">
                            <h5 class="card-title">ç¸½æ–°èæ•¸</h5>
                            <div class="stats-number" id="totalNews">0</div>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card stats-card">
                        <div class="card-body text-center">
                            <h5 class="card-title">ä¸»é¡Œæ•¸é‡</h5>
                            <div class="stats-number" id="totalTopics">0</div>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card stats-card">
                        <div class="card-body text-center">
                            <h5 class="card-title">ä»Šæ—¥æ–°è</h5>
                            <div class="stats-number" id="todayNews">0</div>
                        </div>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="card stats-card">
                        <div class="card-body text-center">
                            <h5 class="card-title">ç†±é–€ä¸»é¡Œ</h5>
                            <div class="stats-number" id="hotTopic">-</div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- æ§åˆ¶é¢æ¿ -->
            <div class="row">
                <div class="col-md-4">
                    <div class="card">
                        <div class="card-header">
                            <h5>ğŸ” æ–°èçˆ¬å–è¨­å®š</h5>
                        </div>
                        <div class="card-body">
                            <div class="mb-3">
                                <label class="form-label">ä¸»é¡Œé—œéµè©</label>
                                <input type="text" class="form-control" id="keyword" placeholder="è¼¸å…¥é—œéµè©ï¼Œå¦‚ï¼šå·æ™®ã€ç¶“æ¿Ÿã€ç§‘æŠ€">
                            </div>
                            <div class="mb-3">
                                <label class="form-label">é–‹å§‹æ—¥æœŸ</label>
                                <input type="date" class="form-control" id="startDate">
                            </div>
                            <div class="mb-3">
                                <label class="form-label">çµæŸæ—¥æœŸ</label>
                                <input type="date" class="form-control" id="endDate">
                            </div>
                            <button class="btn btn-crawl btn-lg w-100" onclick="startCrawling()">
                                <span class="loading">â³ çˆ¬å–ä¸­...</span>
                                <span class="normal">ğŸš€ é–‹å§‹çˆ¬å–</span>
                            </button>
                        </div>
                    </div>
                </div>

                <div class="col-md-8">
                    <div class="card">
                        <div class="card-header d-flex justify-content-between align-items-center">
                            <h5>ğŸ“° æ–°èåˆ—è¡¨</h5>
                            <div>
                                <select class="form-select" id="topicFilter" onchange="filterByTopic()">
                                    <option value="">æ‰€æœ‰ä¸»é¡Œ</option>
                                </select>
                            </div>
                        </div>
                        <div class="card-body">
                            <div class="news-list-container" id="newsList">
                                <p class="text-muted text-center">è«‹è¼¸å…¥é—œéµè©ä¸¦é»æ“Šã€Œé–‹å§‹çˆ¬å–ã€ä¾†ç²å–æ–°è</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- ä¸»é¡Œåˆ†æ -->
            <div class="row mt-4">
                <div class="col-12">
                    <div class="card">
                        <div class="card-header">
                            <h5>ğŸ“Š ä¸»é¡Œåˆ†æ</h5>
                        </div>
                        <div class="card-body">
                            <div id="topicAnalysis">
                                <p class="text-muted text-center">çˆ¬å–æ–°èå¾Œå°‡é¡¯ç¤ºä¸»é¡Œåˆ†æ</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <script>
            // è¨­ç½®é»˜èªæ—¥æœŸ
            document.getElementById('startDate').value = new Date().toISOString().split('T')[0];
            document.getElementById('endDate').value = new Date().toISOString().split('T')[0];

            // è¼‰å…¥çµ±è¨ˆæ•¸æ“š
            loadStats();
            loadTopics();

            function startCrawling() {
                const keyword = document.getElementById('keyword').value.trim();
                if (!keyword) {
                    alert('è«‹è¼¸å…¥é—œéµè©ï¼');
                    return;
                }

                const startDate = document.getElementById('startDate').value;
                const endDate = document.getElementById('endDate').value;

                // é¡¯ç¤ºè¼‰å…¥ç‹€æ…‹
                document.querySelector('.loading').style.display = 'inline';
                document.querySelector('.normal').style.display = 'none';

                // ç™¼é€çˆ¬å–è«‹æ±‚
                fetch('/api/crawl', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        keyword: keyword,
                        start_date: startDate,
                        end_date: endDate
                    })
                })
                .then(response => response.json())
                .then(data => {
                    console.log('çˆ¬å–é–‹å§‹:', data);
                    // ç­‰å¾…3ç§’å¾Œé‡æ–°è¼‰å…¥æ•¸æ“š
                    setTimeout(() => {
                        loadStats();
                        loadNews();
                        loadTopics();
                        loadTopicAnalysis();
                        
                        // æ¢å¾©æŒ‰éˆ•ç‹€æ…‹
                        document.querySelector('.loading').style.display = 'none';
                        document.querySelector('.normal').style.display = 'inline';
                    }, 3000);
                })
                .catch(error => {
                    console.error('éŒ¯èª¤:', error);
                    alert('çˆ¬å–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦');
                    
                    // æ¢å¾©æŒ‰éˆ•ç‹€æ…‹
                    document.querySelector('.loading').style.display = 'none';
                    document.querySelector('.normal').style.display = 'inline';
                });
            }

            function loadStats() {
                fetch('/api/stats')
                .then(response => response.json())
                .then(data => {
                    document.getElementById('totalNews').textContent = data.total_news || 0;
                    document.getElementById('totalTopics').textContent = data.total_topics || 0;
                    document.getElementById('todayNews').textContent = data.today_news || 0;
                    document.getElementById('hotTopic').textContent = data.hot_topic || '-';
                });
            }

            function loadNews() {
                console.log('ğŸ”„ é–‹å§‹è¼‰å…¥æ–°è...');
                fetch('/api/news?per_page=20')
                .then(response => {
                    console.log('ğŸ“¡ APIå›æ‡‰ç‹€æ…‹:', response.status);
                    return response.json();
                })
                .then(data => {
                    console.log('ğŸ“° æ”¶åˆ°æ–°èè³‡æ–™:', data);
                    console.log('ğŸ“Š æ–°èæ•¸é‡:', data.length);
                    const newsList = document.getElementById('newsList');
                    if (data.length === 0) {
                        console.log('âŒ æ²’æœ‰æ–°èè³‡æ–™ï¼Œé¡¯ç¤ºç©ºç‹€æ…‹');
                        newsList.innerHTML = '<p class="text-muted text-center">æš«ç„¡æ–°èè³‡æ–™</p>';
                        return;
                    }
                    console.log('âœ… é–‹å§‹æ¸²æŸ“æ–°èåˆ—è¡¨');

                    newsList.innerHTML = data.map((article, index) => `
                        <div class="news-item">
                            <div class="d-flex justify-content-between align-items-start">
                                <div class="flex-grow-1">
                                    <a href="${article.url}" target="_blank" class="news-title">
                                        ${index + 1}. ${article.source} â†’ ${article.title}
                                    </a>
                                    <div class="news-meta">
                                        <span class="badge bg-primary topic-badge">${article.topic || 'æœªåˆ†é¡'}</span>
                                        <span class="badge bg-secondary">${article.source}</span>
                                        <span class="text-muted">${new Date(article.publish_date).toLocaleDateString()}</span>
                                    </div>
                                    <div class="news-content">
                                        <strong>å…§å®¹æ‘˜è¦ï¼š</strong><br>
                                        ${article.content ? article.content.substring(0, 150) + '...' : 'é»æ“Šæ¨™é¡ŒæŸ¥çœ‹å®Œæ•´å…§å®¹'}
                                    </div>
                                </div>
                                <div class="ms-3">
                                    <a href="${article.url}" target="_blank" class="btn btn-sm btn-outline-primary">
                                        <i class="fas fa-external-link-alt"></i> é–‹å•Ÿ
                                    </a>
                                </div>
                            </div>
                        </div>
                    `).join('');
                    console.log('ğŸ‰ æ–°èåˆ—è¡¨æ¸²æŸ“å®Œæˆ');
                })
                .catch(error => {
                    console.error('âŒ è¼‰å…¥æ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤:', error);
                    const newsList = document.getElementById('newsList');
                    newsList.innerHTML = '<p class="text-danger text-center">è¼‰å…¥æ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤</p>';
                });
            }

            function loadTopics() {
                fetch('/api/topics')
                .then(response => response.json())
                .then(data => {
                    const topicFilter = document.getElementById('topicFilter');
                    topicFilter.innerHTML = '<option value="">æ‰€æœ‰ä¸»é¡Œ</option>' +
                        data.map(topic => `<option value="${topic.name}">${topic.name} (${topic.count})</option>`).join('');
                });
            }

            function loadTopicAnalysis() {
                fetch('/api/topics')
                .then(response => response.json())
                .then(data => {
                    const topicAnalysis = document.getElementById('topicAnalysis');
                    if (data.length === 0) {
                        topicAnalysis.innerHTML = '<p class="text-muted text-center">æš«ç„¡ä¸»é¡Œè³‡æ–™</p>';
                        return;
                    }

                    topicAnalysis.innerHTML = data.map(topic => `
                        <div class="d-inline-block me-3 mb-2">
                            <span class="badge bg-info fs-6">${topic.name}</span>
                            <span class="text-muted">(${topic.count}ç¯‡)</span>
                        </div>
                    `).join('');
                });
            }

            function filterByTopic() {
                const topic = document.getElementById('topicFilter').value;
                const url = topic ? `/api/news?per_page=20&topic=${encodeURIComponent(topic)}` : '/api/news?per_page=20';
                
                fetch(url)
                .then(response => response.json())
                .then(data => {
                    const newsList = document.getElementById('newsList');
                    if (data.length === 0) {
                        newsList.innerHTML = '<p class="text-muted text-center">è©²ä¸»é¡Œæš«ç„¡æ–°è</p>';
                        return;
                    }

                    newsList.innerHTML = data.map((article, index) => `
                        <div class="news-item">
                            <div class="d-flex justify-content-between align-items-start">
                                <div class="flex-grow-1">
                                    <a href="${article.url}" target="_blank" class="news-title">
                                        ${index + 1}. ${article.source} â†’ ${article.title}
                                    </a>
                                    <div class="news-meta">
                                        <span class="badge bg-primary topic-badge">${article.topic || 'æœªåˆ†é¡'}</span>
                                        <span class="badge bg-secondary">${article.source}</span>
                                        <span class="text-muted">${new Date(article.publish_date).toLocaleDateString()}</span>
                                    </div>
                                    <div class="news-content">
                                        <strong>å…§å®¹æ‘˜è¦ï¼š</strong><br>
                                        ${article.content ? article.content.substring(0, 150) + '...' : 'é»æ“Šæ¨™é¡ŒæŸ¥çœ‹å®Œæ•´å…§å®¹'}
                                    </div>
                                </div>
                                <div class="ms-3">
                                    <a href="${article.url}" target="_blank" class="btn btn-sm btn-outline-primary">
                                        <i class="fas fa-external-link-alt"></i> é–‹å•Ÿ
                                    </a>
                                </div>
                            </div>
                        </div>
                    `).join('');
                });
            }

            // åˆå§‹è¼‰å…¥æ–°è
            loadNews();
        </script>
    </body>
    </html>
    '''

@app.route('/api/stats')
def get_stats():
    """ç²å–çµ±è¨ˆæ•¸æ“š"""
    total_news = NewsArticle.query.count()
    total_topics = db.session.query(NewsArticle.topic).filter(NewsArticle.topic.isnot(None)).distinct().count()
    today = datetime.now().date()
    today_news = NewsArticle.query.filter(db.func.date(NewsArticle.publish_date) == today).count()
    
    # æœ€ç†±é–€ä¸»é¡Œ
    hot_topic = db.session.query(
        NewsArticle.topic,
        db.func.count(NewsArticle.id).label('count')
    ).filter(NewsArticle.topic.isnot(None)).group_by(NewsArticle.topic).order_by(db.desc('count')).first()
    
    return jsonify({
        'total_news': total_news,
        'total_topics': total_topics,
        'today_news': today_news,
        'hot_topic': hot_topic[0] if hot_topic else None
    })

@app.route('/api/news')
def get_news():
    """ç²å–æ–°èåˆ—è¡¨"""
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 20, type=int)
    topic = request.args.get('topic')
    
    query = NewsArticle.query
    if topic:
        query = query.filter(NewsArticle.topic == topic)
    
    articles = query.order_by(NewsArticle.publish_date.desc()).limit(per_page).all()
    
    return jsonify([{
        'id': article.id,
        'title': article.title,
        'content': article.content,
        'source': article.source,
        'url': article.url,
        'publish_date': article.publish_date.isoformat(),
        'topic': article.topic,
        'keywords': article.keywords
    } for article in articles])

@app.route('/api/topics')
def get_topics():
    """ç²å–ä¸»é¡Œåˆ—è¡¨"""
    topics = db.session.query(
        NewsArticle.topic,
        db.func.count(NewsArticle.id).label('count')
    ).filter(NewsArticle.topic.isnot(None)).group_by(NewsArticle.topic).all()
    
    return jsonify([{'name': topic[0], 'count': topic[1]} for topic in topics])

@app.route('/api/crawl', methods=['POST'])
def start_crawl():
    """é–‹å§‹çˆ¬å–æ–°è"""
    data = request.get_json()
    keyword = data.get('keyword', '')
    start_date = datetime.strptime(data['start_date'], '%Y-%m-%d')
    end_date = datetime.strptime(data['end_date'], '%Y-%m-%d')
    
    # åœ¨èƒŒæ™¯åŸ·è¡Œçˆ¬å–ä»»å‹™
    thread = threading.Thread(target=run_crawling, args=(keyword, start_date, end_date))
    thread.daemon = True
    thread.start()
    
    return jsonify({'status': 'started', 'message': f'æ­£åœ¨çˆ¬å–é—œæ–¼ã€Œ{keyword}ã€çš„æ–°è...'})

def run_crawling(keyword, start_date, end_date):
    """åŸ·è¡ŒçœŸå¯¦çš„çˆ¬å–ä»»å‹™"""
    with app.app_context():
        try:
            print(f"ğŸš€ é–‹å§‹çˆ¬å–é—œéµè©: {keyword}")
            
            # å…ˆæ¸…é™¤èˆŠçš„æ–°èè³‡æ–™
            print("ğŸ—‘ï¸ æ¸…é™¤èˆŠçš„æ–°èè³‡æ–™...")
            NewsArticle.query.delete()
            db.session.commit()
            print("âœ… èˆŠè³‡æ–™å·²æ¸…é™¤")
            
            # å‰µå»ºçˆ¬èŸ²å¯¦ä¾‹
            crawler = RealNewsCrawler()
            
            # çˆ¬å–æ–°è
            articles = crawler.crawl_news(keyword, max_articles=20, start_date=start_date, end_date=end_date)
            
            print(f"ğŸ“° æˆåŠŸçˆ¬å–åˆ° {len(articles)} ç¯‡æ–°è")
            
            # å„²å­˜åˆ°è³‡æ–™åº«
            for article in articles:
                news_article = NewsArticle(
                    title=article['title'],
                    content=article['content'],
                    source=article['source'],
                    url=article['url'],
                    publish_date=article['publish_date'],
                    topic=article['topic'],
                    keywords=article['keywords']
                )
                db.session.add(news_article)
            
            db.session.commit()
            print(f"âœ… æˆåŠŸå„²å­˜ {len(articles)} ç¯‡æ–°èåˆ°è³‡æ–™åº«")
            
        except Exception as e:
            print(f"âŒ çˆ¬å–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()

if __name__ == '__main__':
    print("=" * 60)
    print("ğŸ‰ ä¸­æ–‡æ–°èçˆ¬èŸ²èˆ‡ä¸»é¡Œåˆ†æç³»çµ±")
    print("=" * 60)
    print("æ­£åœ¨åˆå§‹åŒ–æ‡‰ç”¨ç¨‹å¼...")
    
    # å‰µå»ºè³‡æ–™åº«è¡¨
    print("ğŸ“Š å‰µå»ºè³‡æ–™åº«è¡¨...")
    with app.app_context():
        try:
            db.create_all()
            print("âœ… è³‡æ–™åº«è¡¨å‰µå»ºå®Œæˆ")
            
            # é©—è­‰è¡¨æ ¼æ˜¯å¦å‰µå»ºæˆåŠŸ
            from sqlalchemy import inspect
            inspector = inspect(db.engine)
            tables = inspector.get_table_names()
            print(f"ğŸ“‹ å·²å‰µå»ºçš„è¡¨æ ¼: {tables}")
        except Exception as e:
            print(f"âŒ è³‡æ–™åº«è¡¨å‰µå»ºå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
    
    print("ğŸš€ Flask æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•ä¸­...")
    print("ğŸŒ è«‹è¨ªå•: http://localhost:5000")
    print("â¹ï¸  æŒ‰ Ctrl+C åœæ­¢æ‡‰ç”¨ç¨‹å¼")
    print("=" * 60)
    
    app.run(debug=True, host='0.0.0.0', port=5000)
